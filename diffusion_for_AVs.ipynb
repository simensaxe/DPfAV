{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my imports\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# diffusion paper imports\n",
    "import numpy as np\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implemented for DPfAV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.utils import *\n",
    "from datasets.position_dataset import PositionDataset\n",
    "from datasets.vel_steering_dataset import VelSteeringDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffusion paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.modified_resnet import *\n",
    "from models.modified_unet import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize poses in first image\n",
    "def plot_poses_in_image(T_body_front_cam, front_cam_intrinsic, poses, gt_bool=True, color='lime'):\n",
    "    '''\n",
    "    param:poses, in cam_front frame\n",
    "    Plots the poses in the image\n",
    "    '''\n",
    "    \n",
    "    # transform poses to image frame\n",
    "    poses = [[pose[0], pose[1], 0, 1] for pose in poses]\n",
    "    time_axis = np.arange(len(poses))\n",
    "    x_pixel = []\n",
    "    y_pixel = []\n",
    "    for _pose in poses:\n",
    "        if _pose[0] < 0:\n",
    "            continue\n",
    "        pose = front_cam_intrinsic @ (T_body_front_cam @ _pose)[:3]\n",
    "        pose_pixel = pose[:2] / pose[2]\n",
    "        # check if pixel is in image\n",
    "        if pose_pixel[0] < 0 or pose_pixel[0] > 1600 or pose_pixel[1] < 0 or pose_pixel[1] > 900:\n",
    "            print('Pose outside image', _pose)\n",
    "            continue\n",
    "        x_pixel.append(pose_pixel[0])\n",
    "        y_pixel.append(pose_pixel[1])\n",
    "    if not gt_bool:\n",
    "        # plot with intensity\n",
    "        time_axis = np.arange(len(x_pixel))\n",
    "        plt.scatter(x_pixel, y_pixel, c=time_axis, cmap='magma', s=1)\n",
    "    else:\n",
    "        plt.scatter(x_pixel, y_pixel, color=color, s=1)\n",
    "    \n",
    "\n",
    "def visualize_sample(batch, dataset, action_type, pred_actions=None, idx=0):\n",
    "    if pred_actions is None:\n",
    "        pred_actions = torch.empty_like(batch['action'])\n",
    "    # unnormalize image\n",
    "    normalized_tensor_im = batch['image'][0][1]\n",
    "    unnormalized_tensor_im = image_unnormalizer(normalized_tensor_im)\n",
    "\n",
    "    if action_type == 'positions':\n",
    "        unormalized_positions_gt = position_unnormalizer(batch['action'])\n",
    "        unormalized_positions_pred = position_unnormalizer(pred_actions)\n",
    "    else:\n",
    "        unormalized_positions_gt = from_norm_cmds_to_positions(batch['action'])\n",
    "        unormalized_positions_pred = from_norm_cmds_to_positions(pred_actions)\n",
    "\n",
    "    # transformations\n",
    "    T_body_front_cam = dataset.nusc.get_T_body_front_cam(batch['meta_sequence']['sample_token'][0])\n",
    "    front_cam_intrinsic = dataset.nusc.get_front_cam_intrinsics(batch['meta_sequence']['sample_token'][0])\n",
    "    \n",
    "    \n",
    "    # Convert tensor to a PIL image\n",
    "    image = transforms.ToPILImage()(unnormalized_tensor_im).resize((1600, 900))\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.quiver(0, 0, 0, 0, angles='xy', scale_units='xy', scale=0, color='lime', label='Expert trajectory')\n",
    "    plt.quiver(0, 0, 0, 0, angles='xy', scale_units='xy', scale=0, color='purple', label='Predicted trajectory')\n",
    "    plot_poses_in_image(T_body_front_cam,\n",
    "                         front_cam_intrinsic,\n",
    "                         unormalized_positions_gt[0], color='lime')\n",
    "    plot_poses_in_image(T_body_front_cam,\n",
    "                        front_cam_intrinsic,\n",
    "                        unormalized_positions_pred[0], gt_bool=False, color='magma')\n",
    "    plt.imshow(image)\n",
    "    plt.title(r'Ground truth positions Vs. predicted positions')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(f'gt_vs_pred_{idx}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_states(states1, states2, idx, action_type, arrow_length=2):\n",
    "    # need to fix this\n",
    "    plt.figure()\n",
    "    plt.axis(\"equal\")\n",
    "\n",
    "    # Extract x, y, and yaw values for the first set of states\n",
    "    x1 = [state[1] for state in states1]  # Flip x and y axes\n",
    "    y1 = [state[0] for state in states1]  # Flip x and y axes\n",
    "    yaw1 = [state[2] + np.pi / 2 for state in states1]  # Rotate by 90 degrees\n",
    "\n",
    "    # Extract x, y, and yaw values for the second set of states\n",
    "    x2 = [state[1] for state in states2]  # Flip x and y axes\n",
    "    y2 = [state[0] for state in states2]  # Flip x and y axes\n",
    "   \n",
    "    yaw2 = [state[2] + np.pi / 2 for state in states2] # Rotate by 90 degrees\n",
    "    \n",
    "\n",
    "    # Plot arrows for the first set of states in blue\n",
    "    plt.quiver(0, 0, 0, 0, angles='xy', scale_units='xy', scale=0, color='lime', label='Expert trajectory')\n",
    "    for i in range(len(states1)):\n",
    "        plt.quiver(-x1[i], y1[i], np.cos(yaw1[i])*arrow_length, np.sin(yaw1[i])*arrow_length, angles='xy', scale_units='xy', scale=1, color='lime')\n",
    "\n",
    "    # Plot arrows for the second set of states in red\n",
    "    plt.quiver(0, 0, 0, 0, angles='xy', scale_units='xy', scale=0, color='purple', label='Predicted trajectory')\n",
    "    time_axis = np.arange(len(x2))\n",
    "    for i in range(len(states2)):\n",
    "        plt.quiver(-x2[i], y2[i], np.cos(yaw2[i])*arrow_length,\n",
    "                    np.sin(yaw2[i])*arrow_length, angles='xy', \n",
    "                    scale_units='xy', scale=1, color=plt.cm.magma(time_axis[i] / len(time_axis)))\n",
    "\n",
    "    plt.xlabel(r'$y$')\n",
    "    plt.ylabel(r'$x$')\n",
    "    plt.title(r'Position and heading')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.savefig(f'gt_vs_pred_arrows_{idx}.pdf')\n",
    "    plt.show()\n",
    "\n",
    "def plot_positions_bird_eye(batch, pred_actions, action_type, idx):\n",
    "    if action_type == 'positions':\n",
    "        unormalized_actions_gt = position_unnormalizer(batch['action'])[0]\n",
    "        unormalized_positions_gt = [[pose[0], pose[1], pose[2], 1] for pose in unormalized_actions_gt]\n",
    "        unormalized_actions_pred = position_unnormalizer(pred_actions)[0]\n",
    "        unormalized_positions_pred = [[pose[0], pose[1], pose[2], 1] for pose in unormalized_actions_pred]\n",
    "    else:\n",
    "        # velocity and steering angle\n",
    "        unormalized_positions_gt = from_norm_cmds_to_positions(batch['action'])[0]\n",
    "        unormalized_positions_pred = from_norm_cmds_to_positions(pred_actions)[0]\n",
    "        \n",
    "\n",
    "    plot_states(unormalized_positions_gt, unormalized_positions_pred, idx, action_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_horizon = 2\n",
    "pred_horizon = 16\n",
    "action_horizon = 8\n",
    "print('Loading dataset')\n",
    "dataset_path = '/cluster/work/simenmsa/nuscenes'\n",
    "version = 'v1.0-trainval'\n",
    "action_type = 'vel_steer'\n",
    "test = True\n",
    "test_file = 'test_sample_tokens.txt'\n",
    "\n",
    "if action_type == 'positions':\n",
    "    # define dimensions\n",
    "    action_dim = 3\n",
    "    # agent_pos is 3 dimensional\n",
    "    lowdim_obs_dim = action_dim\n",
    "    # define dataset\n",
    "    dataset = PositionDataset(dataset_path, version, obs_horizon, pred_horizon, action_horizon, test, test_file)\n",
    "elif action_type == 'vel_steer':\n",
    "    action_dim = 2\n",
    "    lowdim_obs_dim = action_dim\n",
    "    dataset = VelSteeringDataset(dataset_path, version, obs_horizon, pred_horizon, action_horizon, test, test_file)\n",
    "    \n",
    "else:\n",
    "    print('ERROR: non existing action type:', action_type)\n",
    "    exit()\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=0)\n",
    "print('Dataset loaded, and contains', len(dataset), 'samples')\n",
    "\n",
    "print('Constructing model')\n",
    "# construct ResNet18 encoder\n",
    "# if you have multiple camera views, use seperate encoder weights for each view.\n",
    "vision_encoder = get_resnet('resnet18')\n",
    "# IMPORTANT!\n",
    "# replace all BatchNorm with GroupNorm to work with EMA\n",
    "# performance will tank if you forget to do this!\n",
    "vision_encoder = replace_bn_with_gn(vision_encoder)\n",
    "# ResNet18 has output dim of 512\n",
    "vision_feature_dim = 512\n",
    "\n",
    "# observation feature has 514 dims in total per step\n",
    "obs_dim = vision_feature_dim + lowdim_obs_dim\n",
    "\n",
    "# create network object\n",
    "noise_pred_net = ConditionalUnet1D(\n",
    "    input_dim=action_dim,\n",
    "    global_cond_dim=obs_dim*obs_horizon\n",
    ")\n",
    "# the final arch has 2 parts\n",
    "nets = nn.ModuleDict({\n",
    "    'vision_encoder': vision_encoder,\n",
    "    'noise_pred_net': noise_pred_net\n",
    "})\n",
    "num_diffusion_iters = 100\n",
    "noise_scheduler = DDPMScheduler(\n",
    "num_train_timesteps=num_diffusion_iters,\n",
    "# the choise of beta schedule has big impact on performance\n",
    "# we found squared cosine works the best\n",
    "beta_schedule='squaredcos_cap_v2',\n",
    "# clip output to [-1,1] to improve stability\n",
    "clip_sample=True,\n",
    "# our network predicts noise (instead of denoised action)\n",
    "prediction_type='epsilon'\n",
    ")\n",
    "# device transfer\n",
    "device = torch.device('cuda')\n",
    "_ = nets.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get bacth sample\n",
    "batch = next(iter(dataloader))\n",
    "# visualize sample\n",
    "print(batch['meta_sequence'].keys())\n",
    "sample = dataset.nusc.nusc.get('sample', batch['meta_sequence']['sample_token'][0])\n",
    "print(dataset.nusc.nusc.get('sample_annotation', sample['anns'][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_pretrained = True\n",
    "if load_pretrained:\n",
    "  pt_path = \"/cluster/work/simenmsa/models/checkpoint_epoch_99.pt\"\n",
    "  \n",
    "  checkpoint = torch.load(pt_path, map_location='cuda')\n",
    "  model_state_dict = checkpoint['model_state_dict']  # Access the model's state_dict\n",
    "  ema_nets = nets\n",
    "  # Load the state_dict into your model\n",
    "  ema_nets.load_state_dict(model_state_dict)\n",
    "  print('Pretrained weights loaded.')\n",
    "else:\n",
    "  print(\"Skipped pretrained weight loading.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from time import time\n",
    "from helpers.evaluator import Evaluator \n",
    "\n",
    "evaluator = Evaluator(dataset.nusc.nusc)\n",
    "# progress bar\n",
    "f = IntProgress(min=0, max=len(dataset)/dataloader.batch_size) # instantiate the bar\n",
    "display(f)\n",
    "saved_figs = 0\n",
    "# iterate over test set\n",
    "for batch_idx, data in enumerate(dataloader):\n",
    "    # already normalized\n",
    "    images = np.stack(data['image'])\n",
    "    agent_poses = np.stack(data['agent_pos'])\n",
    "    images = torch.from_numpy(images).to(device, dtype=torch.float32)\n",
    "    agent_poses = torch.from_numpy(agent_poses).to(device, dtype=torch.float32)\n",
    "    with torch.no_grad():\n",
    "        start_time = time()\n",
    "        visual_emb = ema_nets['vision_encoder'](images.flatten(end_dim=1))\n",
    "        visual_features = visual_emb.reshape(*images.shape[:2], -1)\n",
    "        obs_cond = torch.cat([visual_features, agent_poses], dim=-1).view(images.shape[0], -1)\n",
    "        noisy_action = torch.randn((images.shape[0], pred_horizon, action_dim), device=device)\n",
    "        naction = noisy_action\n",
    "\n",
    "        noise_scheduler.set_timesteps(num_diffusion_iters)\n",
    "        for k in noise_scheduler.timesteps:\n",
    "            noise_pred = ema_nets['noise_pred_net'](\n",
    "                sample=naction,\n",
    "                timestep=k,\n",
    "                global_cond=obs_cond)\n",
    "            \n",
    "            # invserse diffusion process\n",
    "            naction = noise_scheduler.step(\n",
    "                model_output=noise_pred,\n",
    "                timestep=k,\n",
    "                sample=naction\n",
    "            ).prev_sample\n",
    "            \n",
    "    naction = naction.detach().cpu()\n",
    "    # naction has shape (batch_size, pred_horizon, action_dim)\n",
    "    # if saved_figs < 4:\n",
    "    #     data_cp = copy.deepcopy(data)\n",
    "    #     # take only first sample in batch\n",
    "    #     naction_cp = copy.deepcopy(naction)\n",
    "    #     plot_positions_bird_eye(data_cp, naction_cp, action_type, saved_figs)\n",
    "    #     visualize_sample(data_cp, dataset, action_type,  pred_actions=naction_cp, idx=saved_figs)\n",
    "            \n",
    "        \n",
    "    #     saved_figs += 1\n",
    "\n",
    "    # calculate loss\n",
    "    if action_type == 'positions':\n",
    "        # transform to positions\n",
    "        unormalized_positions_pred = position_unnormalizer(naction)\n",
    "        unorm_pos_gt = position_unnormalizer(data['action'])\n",
    "        \n",
    "    else:\n",
    "        # transform to positions\n",
    "        unormalized_positions_pred = from_norm_cmds_to_positions(naction)\n",
    "        unorm_pos_gt = position_unnormalizer(data['gt_pos'])\n",
    "        \n",
    "    # increment progress bar\n",
    "    for pred, batch in zip(unormalized_positions_pred, unorm_pos_gt):\n",
    "        pred_cp = copy.deepcopy(pred)\n",
    "        gt_cp = copy.deepcopy(batch)\n",
    "        evaluator.calc_metrics(gt_cp, pred_cp, data['meta_sequence']['sample_token'][0],\n",
    "                               data['meta_sequence']['T_s_g'][0])\n",
    "    \n",
    "    f.value += 1\n",
    "    if f.value == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator.save_metrics('vel_steer_l2_collision.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serializable visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_body_front_cam = dataset.nusc.get_T_body_front_cam(data['meta_sequence']['sample_token'][0])\n",
    "front_cam_intrinsic = dataset.nusc.get_front_cam_intrinsics(data['meta_sequence']['sample_token'][0])\n",
    "\n",
    "normalized_tensor_im = data['image'][0][1]\n",
    "unnormalized_tensor_im = image_unnormalizer(normalized_tensor_im)\n",
    "# Convert tensor to a PIL image\n",
    "\n",
    "for nac_outer in intermediate_naction:\n",
    "\n",
    "    for j, pred in enumerate(nac_outer):\n",
    "        if j > 50:\n",
    "            image = transforms.ToPILImage()(unnormalized_tensor_im).resize((1600, 900))\n",
    "            pred_cp = copy.deepcopy(pred)\n",
    "            plt.clf()\n",
    "            plt.imshow(image)\n",
    "            plt.axis('off')\n",
    "            plt.quiver(0, 0, 0, 0, angles='xy', scale_units='xy', scale=0, label=r'$k={}$'.format(j))\n",
    "            plot_poses_in_image(T_body_front_cam,\n",
    "                        front_cam_intrinsic,\n",
    "                        pred_cp[0], gt_bool=False, color='magma')\n",
    "            plt.imshow(image)\n",
    "            plt.title(r'Denoising of trajectory')\n",
    "            plt.legend(loc='upper right', markerscale=0, handlelength=0)\n",
    "            plt.savefig(f'denoising{j}.pdf')\n",
    "        #plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffusion_nuscenes",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
